{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b936ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used in this project implementation\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import h5py\n",
    "import math\n",
    "import csv\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a307f8f8",
   "metadata": {},
   "source": [
    "Create Client Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7cb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(data_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(data_list)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cc92c35",
   "metadata": {},
   "source": [
    "Create Dataset through sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9e8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(client_data, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(client_data)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "099a1b3c",
   "metadata": {},
   "source": [
    "Class for Model training, aggregators methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20666943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(51)\n",
    "        np.random.seed(51)\n",
    "        model = tf.keras.models.Sequential([ tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), \n",
    "                                                                    input_shape=[None]), tf.keras.layers.SimpleRNN(400, return_sequences=True), \n",
    "                                            tf.keras.layers.SimpleRNN(400), tf.keras.layers.Dense(1), ])\n",
    "        return model\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    global_count = sum([len(clients_trn_data[client_nam]) for client_nam in client_names])\n",
    "    local_count = len(clients_trn_data[client_name])\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(data, model, window_size):\n",
    "    #forecast (predictions) for every point in the time series\n",
    "    series_trans = np.array(data)\n",
    "    forecast = []\n",
    "    for time in range(min(100,len(data)-window_size)):\n",
    "        forecast.append(model.predict(series_trans[time:time + window_size][np.newaxis]))\n",
    "    forecast = forecast[:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "    print(\"MSE on the validation dataset: \", tf.keras.metrics.mean_squared_error(series_trans[window_size:min(100+window_size,len(data))], results).numpy())\n",
    "    print(\"MAE on the validation dataset: \", tf.keras.metrics.mean_absolute_error(series_trans[window_size:min(100+window_size,len(data))], results).numpy())\n",
    "    return series_trans[window_size:min(100+window_size,len(data))], results\n",
    "\n",
    "def mean_weights(weight_list):\n",
    "    '''Return the mean of the weights'''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.mean(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def median_weights(weight_list):\n",
    "    '''Return the median of weights. '''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.median(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def trim_weights(weight_list, a):\n",
    "    '''Return the mean of of weights without the highest a and lowest a values. '''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.mean(np.sort(grad_list_tuple)[a:-a], axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def encrypt_weights(model, a, context):\n",
    "    l = scale_model_weights(model.get_weights(),a)\n",
    "    enc_weights = []\n",
    "    for j in range(len(l)):\n",
    "        if len(l[j].shape)==1:\n",
    "            enc_weights.append(ts.ckks_vector(ctx_eval, l[j]))\n",
    "        else:\n",
    "            t = []\n",
    "            for i in range(len(l[j])):\n",
    "                t.append(ts.ckks_vector(ctx_eval, l[j][i]))\n",
    "            enc_weights.append(t)\n",
    "    return enc_weights\n",
    "\n",
    "\n",
    "def decrypt_weights(enc_weights, context):\n",
    "    s = []\n",
    "    for j in range(len(enc_weights)):\n",
    "        if type(enc_weights[j]) != list:\n",
    "            s.append(np.array((enc_weights[j].decrypt())))\n",
    "        else:\n",
    "            a = []\n",
    "            for i in range(len(enc_weights[j])):\n",
    "                a.append(np.array((enc_weights[j][i]).decrypt()))\n",
    "            s.append(np.array(a))\n",
    "    return s\n",
    "\n",
    "\n",
    "def add_noise(weights, context):\n",
    "    s = []\n",
    "    l = []\n",
    "    for j in range(len(weights)):\n",
    "        if type(weights[j]) != list:\n",
    "            a = random.uniform(0, 1)\n",
    "            s.append(((weights[j])+a))\n",
    "            l.append((a))\n",
    "        else:\n",
    "            a = random.uniform(0, 1)\n",
    "            t1 = []\n",
    "            t2 = []\n",
    "            for i in range(len(weights[j])):\n",
    "                t1.append(((weights[j][i])+a))\n",
    "                t2.append((a))\n",
    "            s.append((t1))\n",
    "            l.append((t2))\n",
    "    return s, l\n",
    "\n",
    "\n",
    "def remove_noise(weights, N, context):\n",
    "    s = []\n",
    "    for j in range(len(weights)):\n",
    "        if type(weights[j]) != list:\n",
    "            A = 0\n",
    "            for l in range(len(N)):\n",
    "                A = A + N[l][j]\n",
    "            s.append(((weights[j])-A))\n",
    "        else:\n",
    "            t1 = []\n",
    "            for i in range(len(weights[j])):\n",
    "                A = 0\n",
    "                for l in range(len(N)):\n",
    "                    A = A + N[l][j][i]\n",
    "                t1.append(((weights[j][i])-A))\n",
    "            s.append((t1))\n",
    "    return s\n",
    "\n",
    "\n",
    "def sum_enc_weights(L):\n",
    "    s = []\n",
    "    for j in range(len(L[0])):\n",
    "        if type(L[0][j]) != list:\n",
    "            z = L[0][j]\n",
    "            for i in range(1,len(L)):\n",
    "                z = z+ L[i][j]\n",
    "            s.append(z)\n",
    "        else:\n",
    "            a = []\n",
    "            for i in range(len(L[0][j])):\n",
    "                z = L[0][j][i]\n",
    "                for k in range(1,len(L)):\n",
    "                    z = z+ L[k][j][i]\n",
    "                a.append(z)\n",
    "            s.append(a)\n",
    "    return s\n",
    "\n",
    "def Training(dataset, epochs =100):\n",
    "    dataset = np.array(dataset)\n",
    "    window_size = 20\n",
    "    batch_size = 16\n",
    "    modl = SimpleMLP()\n",
    "    model = modl.build()\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.9), metrics=[\"mae\"])\n",
    "    datasets = windowed_dataset(dataset, window_size, batch_size, len(dataset))\n",
    "    model.fit(datasets, epochs, verbose=0)\n",
    "    return model\n",
    "\n",
    "def Plots(I, x,y):\n",
    "    X = I\n",
    "\n",
    "    # Plotting both the curves simultaneously\n",
    "    plt.plot(X, y, color='r', label='Forecasted values')\n",
    "    plt.plot(X, x, color='g', label='Original values')\n",
    "\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Traffic volume\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def FedProx(local_model, global_model, mu:int, optimizer, dataset, loss_fn):\n",
    "    \"\"\"Train `model` on one epoch of `train_data`\"\"\"\n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=3e-4)\n",
    "#     loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    for step, (x,y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = local_model(x)\n",
    "            loss = loss_fn(y, y_hat)\n",
    "            distance = squared_distance(weight_to_vector(local_model.get_weights()), weight_to_vector(global_model.get_weights()))\n",
    "            loss+=mu/2*distance\n",
    "            gradients = tape.gradient(loss, local_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients,local_model.trainable_weights))\n",
    "    return local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data_sets(dataset, num_datasets=8, initial='test'):\n",
    "\n",
    "    #create a list of client names\n",
    "    test_names = ['{}_{}'.format(initial, i+1) for i in range(num_datasets)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(dataset)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_datasets\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_datasets, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(test_names))\n",
    "\n",
    "    return {test_names[i] : shards[i] for i in range(len(test_names))} \n",
    "\n",
    "# data is a list of 10 dataset\n",
    "def get_average_mse(data_set, model, window_size, num_datasets):\n",
    "    MSE = 0\n",
    "    data = create_test_data_sets(data_set, num_datasets)\n",
    "    for test in list(data):\n",
    "        #forecast (predictions) for every point in the time series\n",
    "        series_trans = np.array(data[test])\n",
    "        forecast = []\n",
    "        for time in range(len(series_trans)-window_size):\n",
    "            forecast.append(model.predict(series_trans[time:time + window_size][np.newaxis]))\n",
    "        forecast = forecast[:]\n",
    "        results = np.array(forecast)[:, 0, 0]\n",
    "        MSE += tf.keras.metrics.mean_squared_error(series_trans[window_size:len(series_trans)], results).numpy()\n",
    "    return np.mean(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def krum_aggregation(gradients,nbselected,nbworkers, nbbyzwrks):\n",
    "    \"\"\" Aggregate the gradient using the associated (deprecated) native helper.\n",
    "    Args:\n",
    "      gradients List of submitted gradients, as numpy arrays\n",
    "    Returns:\n",
    "      Aggregated gradient, as a numpy array\n",
    "    \"\"\"\n",
    "    if nbselected == nbworkers:\n",
    "      # Fast path average\n",
    "      result = list()\n",
    "      for i in range(nbworkers):\n",
    "        result.append(gradients[i])\n",
    "      return sum_scaled_weights(result)\n",
    "    else:\n",
    "      # Compute list of scores\n",
    "      scores = [list() for i in range(nbworkers)]\n",
    "      for i in range(nbworkers - 1):\n",
    "        score = scores[i]\n",
    "        for j in range(i + 1, nbworkers):\n",
    "          # With: 0 <= i < j < nbworkers\n",
    "          distance = squared_distance(weight_to_vector(gradients[i]), weight_to_vector(gradients[j]))\n",
    "          if math.isnan(distance):\n",
    "            distance = math.inf\n",
    "          score.append(distance)\n",
    "          scores[j].append(distance)\n",
    "      nbinscore = nbworkers - nbbyzwrks - 2\n",
    "      for i in range(nbworkers):\n",
    "        score = scores[i]\n",
    "        score.sort()\n",
    "        scores[i] = sum(score[:nbinscore])\n",
    "      # Return the average of the m gradients with the smallest score\n",
    "      pairs = [(gradients[i], scores[i]) for i in range(nbworkers)]\n",
    "      pairs.sort(key=lambda pair: pair[1])\n",
    "      result= list()\n",
    "      for i in range(nbselected):\n",
    "        result.append(pairs[i][0])\n",
    "      print(len(result))\n",
    "      return sum_scaled_weights(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
