{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to be imported\n",
    "\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from ipynb.fs.full.Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Initialization\n",
    "time_step =[]\n",
    "Mbits_transmitted = []\n",
    "\n",
    "# Data for training.\n",
    "with open('./Traffic_Train_Data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter = ',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        Mbits_transmitted.append(float(row[1]))\n",
    "        time_step.append(int(row[0]))\n",
    "\n",
    "#converting the lists into arrays\n",
    "series_trans = np.array(Mbits_transmitted)\n",
    "time = np.array(time_step)\n",
    "Clients = create_clients(series_trans,35, 'clients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 8192\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 30\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build()\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "    Noise = list()\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(Clients)\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        window_size = 20\n",
    "        batch_size = 16\n",
    "        dataset = windowed_dataset(np.array(Clients[client]), window_size, batch_size, len(Clients[client]))        \n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build()\n",
    "        #compiling the model with a mean squared error loss and a stochastic gradient descent optimizer\n",
    "        local_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.9), metrics=[\"mae\"])\n",
    "                \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(dataset, epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(Clients, client)\n",
    "        model_encrypted = encrypt_weights(local_model,scaling_factor, ctx_eval)\n",
    "        noisy_model_weight, noise = add_noise(model_encrypted, ctx_eval)\n",
    "        Noise.append(noise)\n",
    "        scaled_local_weight_list.append(noisy_model_weight)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = decrypt_weights((remove_noise(sum_enc_weights(scaled_local_weight_list),Noise, ctx_eval)),ctx_eval)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing the performance of the model.\n",
    "with open('./Traffic_Test_Data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter = ',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        Mbits_transmitted.append(float(row[1]))\n",
    "        time_step.append(int(row[0]))\n",
    "\n",
    "#converting the lists into arrays\n",
    "series_trans_test = np.array(Mbits_transmitted)\n",
    "\n",
    "# The average MSE on 9 datasets\n",
    "print(get_average_mse(series_trans_test, global_model, window_size, 10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
