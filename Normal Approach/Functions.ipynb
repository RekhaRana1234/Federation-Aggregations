{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b936ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used in this project implementation\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import h5py\n",
    "import math\n",
    "import csv\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a307f8f8",
   "metadata": {},
   "source": [
    "Create Client Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7cb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(data_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(data_list)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cc92c35",
   "metadata": {},
   "source": [
    "Create Dataset through sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9e8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(client_data, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(client_data)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "099a1b3c",
   "metadata": {},
   "source": [
    "Class for Model training, aggregators methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20666943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(51)\n",
    "        np.random.seed(51)\n",
    "        model = tf.keras.models.Sequential([ tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), \n",
    "                                                                    input_shape=[None]), tf.keras.layers.SimpleRNN(400, return_sequences=True), \n",
    "                                            tf.keras.layers.SimpleRNN(400), tf.keras.layers.Dense(1), ])\n",
    "        return model\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    global_count = sum([len(clients_trn_data[client_nam]) for client_nam in client_names])\n",
    "    local_count = len(clients_trn_data[client_name])\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(data, model, window_size):\n",
    "    #forecast (predictions) for every point in the time series\n",
    "    series_trans = np.array(data)\n",
    "    forecast = []\n",
    "    for time in range(min(100,len(data)-window_size)):\n",
    "        forecast.append(model.predict(series_trans[time:time + window_size][np.newaxis]))\n",
    "    forecast = forecast[:]\n",
    "    results = np.array(forecast)[:, 0, 0]\n",
    "    print(\"MSE on the validation dataset: \", tf.keras.metrics.mean_squared_error(series_trans[window_size:min(100+window_size,len(data))], results).numpy())\n",
    "    print(\"MAE on the validation dataset: \", tf.keras.metrics.mean_absolute_error(series_trans[window_size:min(100+window_size,len(data))], results).numpy())\n",
    "    return series_trans[window_size:min(100+window_size,len(data))], results\n",
    "\n",
    "def mean_weights(weight_list):\n",
    "    '''Return the mean of the weights'''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.mean(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def median_weights(weight_list):\n",
    "    '''Return the median of weights. '''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.median(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def trim_weights(weight_list, a):\n",
    "    '''Return the mean of of weights without the highest a and lowest a values. '''\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*weight_list):\n",
    "        layer_mean = np.mean(np.sort(grad_list_tuple)[a:-a], axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def encrypt_weights(model, a, context):\n",
    "    l = scale_model_weights(model.get_weights(),a)\n",
    "    enc_weights = []\n",
    "    for j in range(len(l)):\n",
    "        if len(l[j].shape)==1:\n",
    "            enc_weights.append(ts.ckks_vector(ctx_eval, l[j]))\n",
    "        else:\n",
    "            t = []\n",
    "            for i in range(len(l[j])):\n",
    "                t.append(ts.ckks_vector(ctx_eval, l[j][i]))\n",
    "            enc_weights.append(t)\n",
    "    return enc_weights\n",
    "\n",
    "\n",
    "def decrypt_weights(enc_weights, context):\n",
    "    s = []\n",
    "    for j in range(len(enc_weights)):\n",
    "        if type(enc_weights[j]) != list:\n",
    "            s.append(np.array((enc_weights[j].decrypt())))\n",
    "        else:\n",
    "            a = []\n",
    "            for i in range(len(enc_weights[j])):\n",
    "                a.append(np.array((enc_weights[j][i]).decrypt()))\n",
    "            s.append(np.array(a))\n",
    "    return s\n",
    "\n",
    "\n",
    "def add_noise(weights, context):\n",
    "    s = []\n",
    "    l = []\n",
    "    for j in range(len(weights)):\n",
    "        if type(weights[j]) != list:\n",
    "            a = random.uniform(0, 1)\n",
    "            s.append(((weights[j])+a))\n",
    "            l.append((a))\n",
    "        else:\n",
    "            a = random.uniform(0, 1)\n",
    "            t1 = []\n",
    "            t2 = []\n",
    "            for i in range(len(weights[j])):\n",
    "                t1.append(((weights[j][i])+a))\n",
    "                t2.append((a))\n",
    "            s.append((t1))\n",
    "            l.append((t2))\n",
    "    return s, l\n",
    "\n",
    "\n",
    "def remove_noise(weights, N, context):\n",
    "    s = []\n",
    "    for j in range(len(weights)):\n",
    "        if type(weights[j]) != list:\n",
    "            A = 0\n",
    "            for l in range(len(N)):\n",
    "                A = A + N[l][j]\n",
    "            s.append(((weights[j])-A))\n",
    "        else:\n",
    "            t1 = []\n",
    "            for i in range(len(weights[j])):\n",
    "                A = 0\n",
    "                for l in range(len(N)):\n",
    "                    A = A + N[l][j][i]\n",
    "                t1.append(((weights[j][i])-A))\n",
    "            s.append((t1))\n",
    "    return s\n",
    "\n",
    "\n",
    "def sum_enc_weights(L):\n",
    "    s = []\n",
    "    for j in range(len(L[0])):\n",
    "        if type(L[0][j]) != list:\n",
    "            z = L[0][j]\n",
    "            for i in range(1,len(L)):\n",
    "                z = z+ L[i][j]\n",
    "            s.append(z)\n",
    "        else:\n",
    "            a = []\n",
    "            for i in range(len(L[0][j])):\n",
    "                z = L[0][j][i]\n",
    "                for k in range(1,len(L)):\n",
    "                    z = z+ L[k][j][i]\n",
    "                a.append(z)\n",
    "            s.append(a)\n",
    "    return s\n",
    "\n",
    "def Training(dataset, epochs =100):\n",
    "    dataset = np.array(dataset)\n",
    "    window_size = 20\n",
    "    batch_size = 16\n",
    "    modl = SimpleMLP()\n",
    "    model = modl.build()\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.9), metrics=[\"mae\"])\n",
    "    datasets = windowed_dataset(dataset, window_size, batch_size, len(dataset))\n",
    "    model.fit(datasets, epochs, verbose=0)\n",
    "    return model\n",
    "\n",
    "def Plots(I, x,y):\n",
    "    X = I\n",
    "\n",
    "    # Plotting both the curves simultaneously\n",
    "    plt.plot(X, y, color='r', label='Forecasted values')\n",
    "    plt.plot(X, x, color='g', label='Original values')\n",
    "\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Traffic volume\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
